{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify as True if highest during n future days reaches the target thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.helper_functions as hf\n",
    "import pandas as pd\n",
    "\n",
    "db_file_path = './db/ohlcv_ntickers_1254_2000-08-01_to_2023-12-23.pkl'\n",
    "\n",
    "start_date = '2013-01-01'\n",
    "\n",
    "buying_time = 'Open'\n",
    "selling_time = 'High'\n",
    "target_future_days = 5\n",
    "thresholds = [1.1, 1.05, 1.01, 1]\n",
    "cumulated_probs_target = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1CALL.MI</th>\n",
       "      <th>2020.OL</th>\n",
       "      <th>5PG.OL</th>\n",
       "      <th>A2A.MI</th>\n",
       "      <th>A3M.MC</th>\n",
       "      <th>AAK.ST</th>\n",
       "      <th>AALB.AS</th>\n",
       "      <th>AB.PA</th>\n",
       "      <th>ABB.ST</th>\n",
       "      <th>ABCA.PA</th>\n",
       "      <th>...</th>\n",
       "      <th>XXL.OL</th>\n",
       "      <th>YAR.OL</th>\n",
       "      <th>YEXR.MC</th>\n",
       "      <th>YIPS.MC</th>\n",
       "      <th>YIV.MI</th>\n",
       "      <th>ZAL.OL</th>\n",
       "      <th>ZAP.OL</th>\n",
       "      <th>ZEAL.CO</th>\n",
       "      <th>ZENA.OL</th>\n",
       "      <th>ZV.MI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-07-24 00:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>2.080</td>\n",
       "      <td>1.6820</td>\n",
       "      <td>3.560</td>\n",
       "      <td>201.800003</td>\n",
       "      <td>38.959999</td>\n",
       "      <td>3.980</td>\n",
       "      <td>418.899994</td>\n",
       "      <td>5.86</td>\n",
       "      <td>...</td>\n",
       "      <td>1.750</td>\n",
       "      <td>402.700012</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>42.0</td>\n",
       "      <td>29.320000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>0.0588</td>\n",
       "      <td>15.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-25 00:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>98.750000</td>\n",
       "      <td>2.090</td>\n",
       "      <td>1.7095</td>\n",
       "      <td>3.600</td>\n",
       "      <td>201.399994</td>\n",
       "      <td>39.450001</td>\n",
       "      <td>4.190</td>\n",
       "      <td>420.399994</td>\n",
       "      <td>5.95</td>\n",
       "      <td>...</td>\n",
       "      <td>1.710</td>\n",
       "      <td>406.700012</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>42.0</td>\n",
       "      <td>29.299999</td>\n",
       "      <td>231.800003</td>\n",
       "      <td>0.0620</td>\n",
       "      <td>15.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-26 00:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>99.699997</td>\n",
       "      <td>2.085</td>\n",
       "      <td>1.6995</td>\n",
       "      <td>3.610</td>\n",
       "      <td>204.600006</td>\n",
       "      <td>39.490002</td>\n",
       "      <td>4.040</td>\n",
       "      <td>421.899994</td>\n",
       "      <td>5.92</td>\n",
       "      <td>...</td>\n",
       "      <td>1.710</td>\n",
       "      <td>411.399994</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>42.0</td>\n",
       "      <td>28.900000</td>\n",
       "      <td>225.399994</td>\n",
       "      <td>0.0620</td>\n",
       "      <td>15.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-27 00:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>100.400002</td>\n",
       "      <td>2.120</td>\n",
       "      <td>1.7100</td>\n",
       "      <td>3.638</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>39.619999</td>\n",
       "      <td>4.070</td>\n",
       "      <td>419.799988</td>\n",
       "      <td>5.90</td>\n",
       "      <td>...</td>\n",
       "      <td>1.710</td>\n",
       "      <td>411.799988</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>42.0</td>\n",
       "      <td>29.660000</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>16.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-28 00:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>101.599998</td>\n",
       "      <td>2.085</td>\n",
       "      <td>1.7100</td>\n",
       "      <td>3.650</td>\n",
       "      <td>204.399994</td>\n",
       "      <td>40.599998</td>\n",
       "      <td>3.975</td>\n",
       "      <td>425.000000</td>\n",
       "      <td>5.90</td>\n",
       "      <td>...</td>\n",
       "      <td>1.642</td>\n",
       "      <td>413.700012</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>44.0</td>\n",
       "      <td>30.680000</td>\n",
       "      <td>220.199997</td>\n",
       "      <td>0.0586</td>\n",
       "      <td>16.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1CALL.MI     2020.OL  5PG.OL  A2A.MI  A3M.MC      AAK.ST  \\\n",
       "Date                                                                            \n",
       "2023-07-24 00:00:00       NaN   97.000000   2.080  1.6820   3.560  201.800003   \n",
       "2023-07-25 00:00:00       NaN   98.750000   2.090  1.7095   3.600  201.399994   \n",
       "2023-07-26 00:00:00       NaN   99.699997   2.085  1.6995   3.610  204.600006   \n",
       "2023-07-27 00:00:00       NaN  100.400002   2.120  1.7100   3.638  205.000000   \n",
       "2023-07-28 00:00:00       NaN  101.599998   2.085  1.7100   3.650  204.399994   \n",
       "\n",
       "                       AALB.AS  AB.PA      ABB.ST  ABCA.PA  ...  XXL.OL  \\\n",
       "Date                                                        ...           \n",
       "2023-07-24 00:00:00  38.959999  3.980  418.899994     5.86  ...   1.750   \n",
       "2023-07-25 00:00:00  39.450001  4.190  420.399994     5.95  ...   1.710   \n",
       "2023-07-26 00:00:00  39.490002  4.040  421.899994     5.92  ...   1.710   \n",
       "2023-07-27 00:00:00  39.619999  4.070  419.799988     5.90  ...   1.710   \n",
       "2023-07-28 00:00:00  40.599998  3.975  425.000000     5.90  ...   1.642   \n",
       "\n",
       "                         YAR.OL  YEXR.MC  YIPS.MC  YIV.MI  ZAL.OL     ZAP.OL  \\\n",
       "Date                                                                           \n",
       "2023-07-24 00:00:00  402.700012     0.65      1.3  0.0220    42.0  29.320000   \n",
       "2023-07-25 00:00:00  406.700012     0.65      1.3  0.0216    42.0  29.299999   \n",
       "2023-07-26 00:00:00  411.399994     0.65      1.3  0.0216    42.0  28.900000   \n",
       "2023-07-27 00:00:00  411.799988     0.65      1.3  0.0216    42.0  29.660000   \n",
       "2023-07-28 00:00:00  413.700012     0.65      1.3  0.0224    44.0  30.680000   \n",
       "\n",
       "                        ZEAL.CO  ZENA.OL  ZV.MI  \n",
       "Date                                             \n",
       "2023-07-24 00:00:00  238.000000   0.0588  15.36  \n",
       "2023-07-25 00:00:00  231.800003   0.0620  15.36  \n",
       "2023-07-26 00:00:00  225.399994   0.0620  15.68  \n",
       "2023-07-27 00:00:00  221.000000   0.0590  16.16  \n",
       "2023-07-28 00:00:00  220.199997   0.0586  16.60  \n",
       "\n",
       "[5 rows x 1252 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(db_file_path)\n",
    "df = hf.get_rows_after_date(df, start_date)\n",
    "\n",
    "df_buy = df[[buying_time]]\n",
    "df_buy = hf.remove_top_column_name(df_buy)\n",
    "\n",
    "df_sell = df[[selling_time]]\n",
    "df_sell = hf.remove_top_column_name(df_sell)\n",
    "\n",
    "df_buy.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_var(df, past_days, future_days):\n",
    "    var = hf.calculate_variations(df, past_days, future_days)\n",
    "    var_stacked = hf.stack(var, f'var_past_{past_days}d_future_{future_days}d')\n",
    "\n",
    "    return var_stacked\n",
    "\n",
    "def min_max_var(df, past_days):\n",
    "    rolling_min = df.rolling(window=past_days + 1, min_periods=1).min()\n",
    "    min_var = df / rolling_min\n",
    "    min_var_stacked = hf.stack(min_var, f'min_var_past_{past_days}d')\n",
    "\n",
    "    rolling_max = df.rolling(window=past_days + 1, min_periods=1).max()\n",
    "    max_var = df / rolling_max\n",
    "    max_var_stacked = hf.stack(max_var, f'max_var_past_{past_days}d')\n",
    "\n",
    "    return min_var_stacked, max_var_stacked\n",
    "\n",
    "def days_since_min_max(df, past_days):\n",
    "    days_since_min = hf.get_days_since_min(df, past_days)\n",
    "    days_since_min_stacked = hf.stack(days_since_min, f'days_since_min_{past_days}d')\n",
    "\n",
    "    days_since_max = hf.get_days_since_max(df, past_days)\n",
    "    days_since_max_stacked = hf.stack(days_since_max, f'days_since_max_{past_days}d')\n",
    "\n",
    "    return days_since_min_stacked, days_since_max_stacked\n",
    "\n",
    "def get_volatility(df, past_days):\n",
    "    volatility = hf.calculate_volatility(df, past_days)\n",
    "    volatility_stacked = hf.stack(volatility, f'volatility_{past_days}d')\n",
    "\n",
    "    return volatility_stacked\n",
    "\n",
    "def classify_var(df_var, thresholds, col_name):\n",
    "    df_thresholds = hf.classify_var(df_var, thresholds)\n",
    "\n",
    "    df_thresholds_stacked = hf.stack(df_thresholds, col_name)\n",
    "    df_thresholds_stacked = df_thresholds_stacked.droplevel(level=-1)\n",
    "\n",
    "    return df_thresholds_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m var_1 \u001b[38;5;241m=\u001b[39m calculate_var(df_buy, past_days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, future_days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m min_var_30, max_var_30 \u001b[38;5;241m=\u001b[39m min_max_var(df_buy, past_days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m min_var_10, max_var_10 \u001b[38;5;241m=\u001b[39m \u001b[43mmin_max_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_buy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m min_var_5, max_var_5 \u001b[38;5;241m=\u001b[39m min_max_var(df_buy, past_days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     10\u001b[0m min_var_2, max_var_2 \u001b[38;5;241m=\u001b[39m min_max_var(df_buy, past_days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mmin_max_var\u001b[0;34m(df, past_days)\u001b[0m\n\u001b[1;32m     12\u001b[0m rolling_max \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39mpast_days \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     13\u001b[0m max_var \u001b[38;5;241m=\u001b[39m df \u001b[38;5;241m/\u001b[39m rolling_max\n\u001b[0;32m---> 14\u001b[0m max_var_stacked \u001b[38;5;241m=\u001b[39m \u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_var_past_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpast_days\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43md\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m min_var_stacked, max_var_stacked\n",
      "File \u001b[0;32m~/code/stock/utils/helper_functions.py:100\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(df, new_col_name)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(df, new_col_name):\n\u001b[0;32m--> 100\u001b[0m     df_stacked \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    101\u001b[0m     df_stacked\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{df_stacked\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m] : new_col_name}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_stacked\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/frame.py:8217\u001b[0m, in \u001b[0;36mDataFrame.stack\u001b[0;34m(self, level, dropna)\u001b[0m\n\u001b[1;32m   8215\u001b[0m     result \u001b[38;5;241m=\u001b[39m stack_multiple(\u001b[38;5;28mself\u001b[39m, level, dropna\u001b[38;5;241m=\u001b[39mdropna)\n\u001b[1;32m   8216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 8217\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   8219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/reshape/reshape.py:604\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(frame, level, dropna)\u001b[0m\n\u001b[1;32m    601\u001b[0m         new_values \u001b[38;5;241m=\u001b[39m _reorder_for_extension_array_stack(new_values, N, K)\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;66;03m# homogeneous, non-EA\u001b[39;00m\n\u001b[0;32m--> 604\u001b[0m         new_values \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;66;03m# non-homogeneous\u001b[39;00m\n\u001b[1;32m    608\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39m_values\u001b[38;5;241m.\u001b[39mravel()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "var_30 = calculate_var(df_buy, past_days=30, future_days=0)\n",
    "var_10 = calculate_var(df_buy, past_days=10, future_days=0)\n",
    "var_5 = calculate_var(df_buy, past_days=5, future_days=0)\n",
    "var_2 = calculate_var(df_buy, past_days=2, future_days=0)\n",
    "var_1 = calculate_var(df_buy, past_days=1, future_days=0)\n",
    "\n",
    "min_var_30, max_var_30 = min_max_var(df_buy, past_days=30)\n",
    "min_var_10, max_var_10 = min_max_var(df_buy, past_days=10)\n",
    "min_var_5, max_var_5 = min_max_var(df_buy, past_days=5)\n",
    "min_var_2, max_var_2 = min_max_var(df_buy, past_days=2)\n",
    "\n",
    "days_since_min_30, days_since_max_30 = days_since_min_max(df_buy, past_days=30)\n",
    "days_since_min_10, days_since_max_10 = days_since_min_max(df_buy, past_days=10)\n",
    "\n",
    "volatility_30 = get_volatility(df_buy, past_days=30)\n",
    "volatility_10 = get_volatility(df_buy, past_days=10)\n",
    "volatility_2 = get_volatility(df_buy, past_days=2)\n",
    "\n",
    "buy_var = calculate_var(df_sell, past_days=0, future_days=target_future_days)\n",
    "buy_class = classify_var(buy_var, thresholds, 'buy_class')\n",
    "\n",
    "input_output_df = pd.concat(\n",
    "        [var_30, var_10, var_5, var_2, var_1,\n",
    "        min_var_30, min_var_10, min_var_5, min_var_2,\n",
    "        max_var_30, max_var_10, max_var_5, max_var_2,\n",
    "        days_since_min_30, days_since_min_10,\n",
    "        days_since_max_30, days_since_max_10,\n",
    "        volatility_30, volatility_10, volatility_2,\n",
    "        buy_class],\n",
    "    axis='columns')\n",
    "\n",
    "input_output_df = input_output_df.dropna()\n",
    "\n",
    "input_columns = [col for col in input_output_df.columns if not col.startswith('buy')]\n",
    "input_df = input_output_df[input_columns]\n",
    "output_df = input_output_df[['buy_class']]\n",
    "\n",
    "input_output_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = output_df['buy_class'].value_counts()\n",
    "percentages = (value_counts / len(output_df)) * 100\n",
    "percentages = percentages.sort_index()\n",
    "cumulative_percentages = percentages.cumsum()\n",
    "\n",
    "print(f'''\n",
    "Percentage of each class:\n",
    "{percentages}\n",
    "\n",
    "Cumulative percentages:\n",
    "{cumulative_percentages}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "\n",
    "X_all = StandardScaler().fit_transform(input_df)\n",
    "y_all = output_df.values.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=0.8, test_size=0.2, random_state=42)\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "last_layers_size = len(thresholds) + 1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(last_layers_size, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "counter = Counter(y_train)\n",
    "max_count = max(counter.values())\n",
    "class_weights = {cls: max_count / count for cls, count in counter.items()}\n",
    "\n",
    "model.fit(X_train, y_train, epochs=2, batch_size=32, validation_data=(X_test, y_test), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict(X_test)\n",
    "df_prediction = pd.DataFrame(y_prediction, columns=['prob_0', 'prob_1', 'prob_2', 'prob_3', 'prob_4'])\n",
    "df_test = pd.DataFrame({'real_class': y_test})\n",
    "df_comparison = pd.concat([df_prediction, df_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison['cumulated_probs'] = df_comparison['prob_0'] + df_comparison['prob_1']\n",
    "df_comparison['predicted_true'] = (df_comparison['cumulated_probs'] > cumulated_probs_target)\n",
    "df_comparison['real_true'] = (df_comparison['real_class'] <= 1)\n",
    "\n",
    "df_comparison.head(5)\n",
    "\n",
    "tp = ((df_comparison['real_true'] == True) & (df_comparison['predicted_true'] == True)).sum()\n",
    "tn = ((df_comparison['real_true'] == False) & (df_comparison['predicted_true'] == False)).sum()\n",
    "fp = ((df_comparison['real_true'] == False) & (df_comparison['predicted_true'] == True)).sum()\n",
    "fn = ((df_comparison['real_true'] == True) & (df_comparison['predicted_true'] == False)).sum()\n",
    "\n",
    "print(f\"True Positives (TP), Correctly bought, earned money: {tp}\")\n",
    "print(f\"True Negatives (TN), Correctly not bought: {tn}\")\n",
    "print(f\"False Positives (FP), Incorrectly bought, may have lost money : {fp}\")\n",
    "print(f\"False Negatives (FN), Missed buying opportunity: {fn}\")\n",
    "\n",
    "winning_rate = tp / (tp + fp)\n",
    "print(f'Winning rate: {round(winning_rate * 100, 2)} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buying_time = 'Open' # 'Open' or 'Close'\n",
    "# target_future_days = 5\n",
    "# thresholds = [1.1, 1.05, 1.01, 1]\n",
    "# cumulated_probs_target = 0.5\n",
    "\n",
    "# True Positives (TP), Correctly bought, earned money: 160\n",
    "# True Negatives (TN), Correctly not bought: 222576\n",
    "# False Positives (FP), Incorrectly bought, may have lost money : 251\n",
    "# False Negatives (FN), Missed buying opportunity: 136939\n",
    "# Winning rate: 38.93 %\n",
    "\n",
    "# buying_time = 'Open'\n",
    "# target_future_days = 10\n",
    "# thresholds = [1.1, 1.05, 1.01, 1]\n",
    "# cumulated_probs_target = 0.6\n",
    "\n",
    "# True Positives (TP), Correctly bought, earned money: 6712\n",
    "# True Negatives (TN), Correctly not bought: 273724\n",
    "# False Positives (FP), Incorrectly bought, may have lost money : 14081\n",
    "# False Negatives (FN), Missed buying opportunity: 63609\n",
    "# Winning rate: 32.28 %"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
