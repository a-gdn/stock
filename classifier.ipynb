{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "import utils.helper_functions as hf\n",
    "import utils.inputs as inputs\n",
    "import utils.outputs as outputs\n",
    "import utils.tf_model as tf_model\n",
    "import utils.evaluate as eval\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import hyperopt\n",
    "from hyperopt import hp, fmin, tpe\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1' # disable file validation in the debugger\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' #0: All logs (default setting), 1: Filter out INFO logs, up to 3\n",
    "pd.options.mode.copy_on_write = True # avoid making unnecessary copies of DataFrames or Series\n",
    "\n",
    "\n",
    "# import config as cfg\n",
    "# import utils.helper_functions as hf\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential # type: ignore\n",
    "# from tensorflow.keras.layers import Dense, BatchNormalization, Dropout # type: ignore\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# from collections import Counter\n",
    "\n",
    "# import os\n",
    "# os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1' # disable file validation in the debugger\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' #0: All logs (default setting), 1: Filter out INFO logs, up to 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.use_hyperopt:\n",
    "    print(cfg.hyperopt_n_iterations)\n",
    "else:\n",
    "    num_combinations = hf.get_num_combinations(cfg.param_grid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(cfg.db_path)\n",
    "df = hf.get_rows_after_date(df, cfg.start_date)\n",
    "df = hf.fillnavalues(df)\n",
    "\n",
    "def get_single_level_df(df, ohlcv):\n",
    "    new_df = df[[ohlcv]]\n",
    "    new_df = hf.remove_top_column_name(new_df)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def get_ohlcv_dfs(df):\n",
    "    df_open = get_single_level_df(df, 'Open')\n",
    "    df_high = get_single_level_df(df, 'High')\n",
    "    df_low = get_single_level_df(df, 'Low')\n",
    "    df_close = get_single_level_df(df, 'Close')\n",
    "    df_volume = get_single_level_df(df, 'Volume')\n",
    "    \n",
    "    return {'df_open': df_open, 'df_high': df_high, 'df_low': df_low,\n",
    "            'df_close': df_close, 'df_volume': df_volume}\n",
    "\n",
    "num_tickers = hf.get_num_tickers(get_single_level_df(df, 'Open'))\n",
    "print(f'number of tickers: {num_tickers}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_data(hyperparams):\n",
    "    df_buy = get_single_level_df(df, hyperparams['buying_time'])\n",
    "    df_sell = get_single_level_df(df, hyperparams['selling_time'])\n",
    "    dfs_ohlcv = get_ohlcv_dfs(df)\n",
    "\n",
    "    if os.path.exists(cfg.transformed_data_path) and cfg.use_saved_transformed_data:\n",
    "        df_data = pd.read_pickle(cfg.transformed_data_path)\n",
    "        print(f'using existing {cfg.transformed_data_path}')\n",
    "    else:\n",
    "        print(f'need to create {cfg.transformed_data_path}')\n",
    "        df_data = inputs.get_inputs(df_buy, dfs_ohlcv, hyperparams['buying_time'])\n",
    "        \n",
    "        df_data.to_pickle(cfg.transformed_data_path)\n",
    "        print(f'saved new {cfg.transformed_data_path}')\n",
    "\n",
    "    df_data = outputs.add_outputs(df_data, df_buy, df_sell, dfs_ohlcv, num_tickers, cfg.output_class_name, cfg.fee, **hyperparams)\n",
    "\n",
    "    df_data = df_data.dropna()\n",
    "\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def get_dfs_input_output(df_data, output_class_name):\n",
    "#     input_columns = [col for col in df_data.columns if col.startswith('input_')]\n",
    "#     df_input = df_data[input_columns]\n",
    "#     df_output = df_data[[output_class_name]]\n",
    "\n",
    "#     return df_input, df_output\n",
    "\n",
    "# def get_test_train_data(df_input, df_output, test_size):\n",
    "#     X_train = df_input[:-test_size].values\n",
    "#     y_train = df_output[:-test_size].values.ravel().astype(int)\n",
    "\n",
    "#     X_test = df_input.tail(test_size).values\n",
    "#     y_test = df_output.tail(test_size).values.ravel().astype(int)\n",
    "\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train = scaler.fit_transform(X_train)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "\n",
    "#     hf.save_object(scaler, './outputs/scaler.pkl')\n",
    "\n",
    "#     print(f\"number of elements in y_train: {len(y_train)}\")\n",
    "#     print(f\"number of elements in y_test: {len(y_test)}\")\n",
    "\n",
    "#     return {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}\n",
    "\n",
    "# def create_model(**kwargs):\n",
    "#     X_train = kwargs.get('X_train')\n",
    "#     X_test = kwargs.get('X_test')\n",
    "#     y_train = kwargs.get('y_train')\n",
    "#     y_test = kwargs.get('y_test')\n",
    "\n",
    "#     thresholds = kwargs.get('thresholds')\n",
    "    \n",
    "#     size_layer_1 = kwargs.get('size_layer_1')\n",
    "#     size_layer_2 = kwargs.get('size_layer_2')\n",
    "#     size_layer_3 = kwargs.get('size_layer_3')\n",
    "#     dropout_rate = kwargs.get('dropout_rate')\n",
    "#     balance_data = kwargs.get('balance_data')\n",
    "#     batch_size = kwargs.get('batch_size')\n",
    "\n",
    "#     last_layers_size = len(thresholds) + 1\n",
    "\n",
    "#     model = Sequential()\n",
    "\n",
    "#     model.add(Dense(size_layer_1, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(size_layer_2, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(size_layer_3, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(last_layers_size, activation='softmax'))\n",
    "\n",
    "#     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     if (balance_data):\n",
    "#         counter = Counter(y_train)\n",
    "#         max_count = max(counter.values())\n",
    "#         class_weights = {cls: max_count / count for cls, count in counter.items()}\n",
    "#         model.fit(X_train, y_train, epochs=cfg.epochs, batch_size=batch_size, validation_data=(X_test, y_test), class_weight=class_weights)\n",
    "#     else:\n",
    "#         model.fit(X_train, y_train, epochs=cfg.epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "\n",
    "#     model.save(cfg.model_path)\n",
    "\n",
    "# def load_model(df_data, hyperparams):\n",
    "#     df_input, df_output = get_dfs_input_output(df_data, cfg.output_class_name)\n",
    "\n",
    "#     test_train_data = get_test_train_data(df_input, df_output, cfg.test_size)\n",
    "\n",
    "#     if os.path.exists(cfg.model_path) and cfg.use_saved_model:\n",
    "#         print(f'using existing {cfg.model_path}')\n",
    "#     else:\n",
    "#         print(f'need to create {cfg.model_path}')\n",
    "#         create_model(**{**test_train_data, **hyperparams})\n",
    "    \n",
    "#     model = tf.keras.models.load_model(cfg.model_path)\n",
    "\n",
    "#     return test_train_data, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "i = 0\n",
    "results = []\n",
    "\n",
    "def objective(hyperparams):\n",
    "    hyperparams['thresholds'] = [hyperparams['thresholds']]\n",
    "    hyperparams['rank_pct_thresholds'] = [hyperparams['rank_pct_thresholds']]\n",
    "    \n",
    "    df_data = get_df_data(hyperparams)\n",
    "    test_train_data, model = model.load_model(df_data, hyperparams)\n",
    "    performance_metrics = eval.evaluate_model(df_data, model, test_train_data, hyperparams)\n",
    "\n",
    "    result = {**performance_metrics, **hyperparams, 'epochs': cfg.epochs}\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "\n",
    "    performance = result['performance_score']\n",
    "\n",
    "    return -performance\n",
    "\n",
    "if cfg.use_hyperopt:\n",
    "    best = fmin(objective, cfg.search_space, algo=tpe.suggest, max_evals=cfg.hyperopt_n_iterations)\n",
    "    print(f'best parameters: {best}')\n",
    "else:\n",
    "    for params in product(*cfg.param_grid.values()):\n",
    "        i += 1\n",
    "        # clear_output(wait=True) # clear printed outputs\n",
    "        hf.print_combination(i, num_combinations)\n",
    "\n",
    "        hyperparams = dict(zip(cfg.param_grid.keys(), params))\n",
    "\n",
    "        df_data = get_df_data(hyperparams)\n",
    "        test_train_data, model = tf_model.load_model(df_data, hyperparams)\n",
    "        performance_metrics = eval.evaluate_model(df_data, model, test_train_data, num_tickers, num_combinations, hyperparams)\n",
    "\n",
    "        result = {**performance_metrics, **hyperparams, 'epochs': cfg.epochs}\n",
    "        print(result)\n",
    "        results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results = df_results.sort_values(by='performance_score', ascending=False)\n",
    "df_results.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_excel(f'./outputs/{hf.get_date()}_classifier_results.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
